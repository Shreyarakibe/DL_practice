Deep Learning Practice Repository:
A collection of hands-on deep learning experiments implemented using Python, TensorFlow/Keras.
This repository contains multiple Jupyter notebooks covering fundamental and advanced DL concepts such as Feedforward Neural Networks, CNNs, Transfer Learning, Autoencoders, and Word Embeddings.

1.FNN.ipynb — Feedforward Neural Network:
-Introduction to neural network basics
-Dense layers, activations (ReLU, sigmoid, softmax)
-Training with backpropagation
-Evaluation on a simple dataset
-Demonstrates overfitting & regularization


2.DL1.ipynb — Deep Learning Basics / Classification:
-Data preprocessing
-Building multi-layer neural networks
-Loss functions & optimizers
-Accuracy measurement
-Visualization of training performance


3.Image_classification_CNN.ipynb — Convolutional Neural Networks:
-CNN architecture (Conv → Pool → Dense)
-Image preprocessing and augmentation
-Training a classifier on a custom or standard dataset
-Improving accuracy with dropout, batch normalization
-Plotting training accuracy & loss


4. Tranfer_learning_VGG16.ipynb — Transfer Learning with VGG16:
-Using VGG16 as a feature extractor
-Freezing and fine-tuning layers
-Using pre-trained ImageNet weights
-Performance comparison: scratch vs pre-trained
-Achieves higher accuracy with less data


5. Autoencoder.ipynb — Autoencoder for Dimensionality Reduction:
-Encoder–Decoder architecture
-Image reconstruction
-Latent space visualization
-Demonstration of noise reduction (denoising autoencoder)


6. CBOW.ipynb — Word Embeddings (NLP):
-Implementing a Continuous Bag-of-Words model
-Training word embeddings
-Understanding context windows
-Vector representation of words
-Basic semantic similarity queries


#Technologies Used:
Python 3.x
TensorFlow / Keras
NumPy
Matplotlib
Scikit-learn
Jupyter Notebook
